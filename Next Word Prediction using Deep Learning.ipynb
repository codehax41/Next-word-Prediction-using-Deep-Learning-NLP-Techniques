{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a521cd5",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a0bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c5d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data and clean row breaks, notations, symbols\n",
    "file = open(\"1661-0.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace(\"”\", '')\n",
    "data = data.split()\n",
    "data = ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7fc1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set en\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5945ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[142, 4680, 1, 986, 5, 125, 33, 46, 556, 2164]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e9bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573660\n",
      "108958\n"
     ]
    }
   ],
   "source": [
    "#Shape after tokenizing\n",
    "print(len(data))\n",
    "print(len(sequence_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59a99d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8624"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The set of unique words used in the text corpus is referred to as the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513f0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sequences : 108955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 142, 4680,    1,  986],\n",
       "       [4680,    1,  986,    5],\n",
       "       [   1,  986,    5,  125],\n",
       "       [ 986,    5,  125,   33],\n",
       "       [   5,  125,   33,   46],\n",
       "       [ 125,   33,   46,  556],\n",
       "       [  33,   46,  556, 2164],\n",
       "       [  46,  556, 2164, 2165],\n",
       "       [ 556, 2164, 2165,   27],\n",
       "       [2164, 2165,   27,  987]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 word sequence\n",
    "sequences = []\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The length of sequences :\", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27e13e",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9462f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data :\n",
      " [[ 142 4680    1]\n",
      " [4680    1  986]\n",
      " [   1  986    5]\n",
      " [ 986    5  125]\n",
      " [   5  125   33]\n",
      " [ 125   33   46]\n",
      " [  33   46  556]\n",
      " [  46  556 2164]\n",
      " [ 556 2164 2165]\n",
      " [2164 2165   27]]\n",
      "\n",
      "Response :\n",
      " [ 986    5  125   33   46  556 2164 2165   27  987]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data :\\n\", X[:10])\n",
    "print()\n",
    "print(\"Response :\\n\", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5a4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0725d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "#Embedding(input_dim,output_dim,\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a13914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             86240     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8624)              8632624   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,767,864\n",
      "Trainable params: 21,767,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8563cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd306d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 6.5776\n",
      "Epoch 1: loss improved from inf to 6.57758, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 247s 353ms/step - loss: 6.5776 - val_loss: 6.5056\n",
      "Epoch 2/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 5.9535\n",
      "Epoch 2: loss improved from 6.57758 to 5.95348, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 226s 332ms/step - loss: 5.9535 - val_loss: 6.2266\n",
      "Epoch 3/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 5.5422\n",
      "Epoch 3: loss improved from 5.95348 to 5.54217, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 232s 340ms/step - loss: 5.5422 - val_loss: 6.2377\n",
      "Epoch 4/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 5.2687\n",
      "Epoch 4: loss improved from 5.54217 to 5.26869, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 233s 343ms/step - loss: 5.2687 - val_loss: 6.3723\n",
      "Epoch 5/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 5.0400\n",
      "Epoch 5: loss improved from 5.26869 to 5.04000, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 229s 336ms/step - loss: 5.0400 - val_loss: 6.4287\n",
      "Epoch 6/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 4.8257\n",
      "Epoch 6: loss improved from 5.04000 to 4.82569, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 231s 339ms/step - loss: 4.8257 - val_loss: 6.6496\n",
      "Epoch 7/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 4.5960\n",
      "Epoch 7: loss improved from 4.82569 to 4.59602, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 232s 340ms/step - loss: 4.5960 - val_loss: 6.9655\n",
      "Epoch 8/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 4.3444\n",
      "Epoch 8: loss improved from 4.59602 to 4.34440, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 231s 339ms/step - loss: 4.3444 - val_loss: 7.4461\n",
      "Epoch 9/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 4.0617\n",
      "Epoch 9: loss improved from 4.34440 to 4.06170, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 233s 342ms/step - loss: 4.0617 - val_loss: 7.8160\n",
      "Epoch 10/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 3.7613\n",
      "Epoch 10: loss improved from 4.06170 to 3.76131, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 228s 335ms/step - loss: 3.7613 - val_loss: 8.4348\n",
      "Epoch 11/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 3.4650\n",
      "Epoch 11: loss improved from 3.76131 to 3.46498, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 234s 344ms/step - loss: 3.4650 - val_loss: 9.1367\n",
      "Epoch 12/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 3.1811\n",
      "Epoch 12: loss improved from 3.46498 to 3.18110, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 269s 395ms/step - loss: 3.1811 - val_loss: 9.9031\n",
      "Epoch 13/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 2.9156\n",
      "Epoch 13: loss improved from 3.18110 to 2.91558, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 233s 343ms/step - loss: 2.9156 - val_loss: 10.4598\n",
      "Epoch 14/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 2.6687\n",
      "Epoch 14: loss improved from 2.91558 to 2.66869, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 232s 341ms/step - loss: 2.6687 - val_loss: 11.4185\n",
      "Epoch 15/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 2.4332\n",
      "Epoch 15: loss improved from 2.66869 to 2.43316, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 248s 365ms/step - loss: 2.4332 - val_loss: 12.1236\n",
      "Epoch 16/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 2.2035\n",
      "Epoch 16: loss improved from 2.43316 to 2.20355, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 254s 373ms/step - loss: 2.2035 - val_loss: 12.7624\n",
      "Epoch 17/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.9890\n",
      "Epoch 17: loss improved from 2.20355 to 1.98903, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 238s 350ms/step - loss: 1.9890 - val_loss: 13.6569\n",
      "Epoch 18/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.7725\n",
      "Epoch 18: loss improved from 1.98903 to 1.77246, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 270s 397ms/step - loss: 1.7725 - val_loss: 14.5506\n",
      "Epoch 19/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.5699\n",
      "Epoch 19: loss improved from 1.77246 to 1.56993, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 236s 347ms/step - loss: 1.5699 - val_loss: 15.1606\n",
      "Epoch 20/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.3834\n",
      "Epoch 20: loss improved from 1.56993 to 1.38341, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 264s 388ms/step - loss: 1.3834 - val_loss: 15.7332\n",
      "Epoch 21/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.2128\n",
      "Epoch 21: loss improved from 1.38341 to 1.21285, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 235s 345ms/step - loss: 1.2128 - val_loss: 16.2769\n",
      "Epoch 22/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 1.0736\n",
      "Epoch 22: loss improved from 1.21285 to 1.07363, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 231s 339ms/step - loss: 1.0736 - val_loss: 16.5786\n",
      "Epoch 23/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.9490\n",
      "Epoch 23: loss improved from 1.07363 to 0.94902, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 236s 347ms/step - loss: 0.9490 - val_loss: 17.4671\n",
      "Epoch 24/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.8551\n",
      "Epoch 24: loss improved from 0.94902 to 0.85509, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 230s 337ms/step - loss: 0.8551 - val_loss: 17.6022\n",
      "Epoch 25/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.7772\n",
      "Epoch 25: loss improved from 0.85509 to 0.77721, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 222s 326ms/step - loss: 0.7772 - val_loss: 17.9451\n",
      "Epoch 26/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.7204\n",
      "Epoch 26: loss improved from 0.77721 to 0.72043, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 220s 324ms/step - loss: 0.7204 - val_loss: 17.7777\n",
      "Epoch 27/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.6778\n",
      "Epoch 27: loss improved from 0.72043 to 0.67780, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 229s 336ms/step - loss: 0.6778 - val_loss: 18.3039\n",
      "Epoch 28/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.6397\n",
      "Epoch 28: loss improved from 0.67780 to 0.63968, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 220s 323ms/step - loss: 0.6397 - val_loss: 17.9457\n",
      "Epoch 29/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.6155\n",
      "Epoch 29: loss improved from 0.63968 to 0.61553, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 203s 299ms/step - loss: 0.6155 - val_loss: 18.2849\n",
      "Epoch 30/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5952\n",
      "Epoch 30: loss improved from 0.61553 to 0.59520, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 289ms/step - loss: 0.5952 - val_loss: 17.9872\n",
      "Epoch 31/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5694\n",
      "Epoch 31: loss improved from 0.59520 to 0.56936, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 208s 305ms/step - loss: 0.5694 - val_loss: 18.0707\n",
      "Epoch 32/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5528\n",
      "Epoch 32: loss improved from 0.56936 to 0.55276, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 290ms/step - loss: 0.5528 - val_loss: 17.8115\n",
      "Epoch 33/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5412\n",
      "Epoch 33: loss improved from 0.55276 to 0.54122, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 200s 293ms/step - loss: 0.5412 - val_loss: 18.1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5290\n",
      "Epoch 34: loss improved from 0.54122 to 0.52900, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 290ms/step - loss: 0.5290 - val_loss: 18.2375\n",
      "Epoch 35/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5122\n",
      "Epoch 35: loss improved from 0.52900 to 0.51224, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 199s 292ms/step - loss: 0.5122 - val_loss: 18.2070\n",
      "Epoch 36/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.5021\n",
      "Epoch 36: loss improved from 0.51224 to 0.50209, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 289ms/step - loss: 0.5021 - val_loss: 17.8890\n",
      "Epoch 37/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4904\n",
      "Epoch 37: loss improved from 0.50209 to 0.49043, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 198s 290ms/step - loss: 0.4904 - val_loss: 18.1973\n",
      "Epoch 38/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4834\n",
      "Epoch 38: loss improved from 0.49043 to 0.48339, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 289ms/step - loss: 0.4834 - val_loss: 17.7580\n",
      "Epoch 39/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4784\n",
      "Epoch 39: loss improved from 0.48339 to 0.47839, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 202s 297ms/step - loss: 0.4784 - val_loss: 18.0089\n",
      "Epoch 40/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4666\n",
      "Epoch 40: loss improved from 0.47839 to 0.46664, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 194s 285ms/step - loss: 0.4666 - val_loss: 18.1774\n",
      "Epoch 41/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4573\n",
      "Epoch 41: loss improved from 0.46664 to 0.45732, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 200s 293ms/step - loss: 0.4573 - val_loss: 17.9159\n",
      "Epoch 42/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4505\n",
      "Epoch 42: loss improved from 0.45732 to 0.45050, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 196s 287ms/step - loss: 0.4505 - val_loss: 18.5587\n",
      "Epoch 43/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4484\n",
      "Epoch 43: loss improved from 0.45050 to 0.44845, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 290ms/step - loss: 0.4484 - val_loss: 17.7877\n",
      "Epoch 44/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4382\n",
      "Epoch 44: loss improved from 0.44845 to 0.43825, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 196s 288ms/step - loss: 0.4382 - val_loss: 18.3823\n",
      "Epoch 45/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4373\n",
      "Epoch 45: loss improved from 0.43825 to 0.43729, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 193s 284ms/step - loss: 0.4373 - val_loss: 18.2072\n",
      "Epoch 46/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4292\n",
      "Epoch 46: loss improved from 0.43729 to 0.42921, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 205s 300ms/step - loss: 0.4292 - val_loss: 18.2090\n",
      "Epoch 47/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4224\n",
      "Epoch 47: loss improved from 0.42921 to 0.42235, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 289ms/step - loss: 0.4224 - val_loss: 18.2965\n",
      "Epoch 48/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4184\n",
      "Epoch 48: loss improved from 0.42235 to 0.41842, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 192s 281ms/step - loss: 0.4184 - val_loss: 18.1997\n",
      "Epoch 49/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4152\n",
      "Epoch 49: loss improved from 0.41842 to 0.41522, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 190s 279ms/step - loss: 0.4152 - val_loss: 19.0517\n",
      "Epoch 50/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4060\n",
      "Epoch 50: loss improved from 0.41522 to 0.40596, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 205s 302ms/step - loss: 0.4060 - val_loss: 17.9610\n",
      "Epoch 51/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4044\n",
      "Epoch 51: loss improved from 0.40596 to 0.40436, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 201s 296ms/step - loss: 0.4044 - val_loss: 18.6261\n",
      "Epoch 52/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4031\n",
      "Epoch 52: loss improved from 0.40436 to 0.40313, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 200s 294ms/step - loss: 0.4031 - val_loss: 18.9197\n",
      "Epoch 53/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3978\n",
      "Epoch 53: loss improved from 0.40313 to 0.39784, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 201s 295ms/step - loss: 0.3978 - val_loss: 18.6541\n",
      "Epoch 54/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3920\n",
      "Epoch 54: loss improved from 0.39784 to 0.39201, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 194s 285ms/step - loss: 0.3920 - val_loss: 18.6622\n",
      "Epoch 55/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3855\n",
      "Epoch 55: loss improved from 0.39201 to 0.38553, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 212s 311ms/step - loss: 0.3855 - val_loss: 18.6661\n",
      "Epoch 56/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3873\n",
      "Epoch 56: loss did not improve from 0.38553\n",
      "681/681 [==============================] - 196s 288ms/step - loss: 0.3873 - val_loss: 18.8136\n",
      "Epoch 57/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3851\n",
      "Epoch 57: loss improved from 0.38553 to 0.38511, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 193s 284ms/step - loss: 0.3851 - val_loss: 18.8052\n",
      "Epoch 58/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3822\n",
      "Epoch 58: loss improved from 0.38511 to 0.38220, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 195s 287ms/step - loss: 0.3822 - val_loss: 18.9191\n",
      "Epoch 59/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3737\n",
      "Epoch 59: loss improved from 0.38220 to 0.37374, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 201s 295ms/step - loss: 0.3737 - val_loss: 18.5077\n",
      "Epoch 60/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3721\n",
      "Epoch 60: loss improved from 0.37374 to 0.37215, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 191s 281ms/step - loss: 0.3721 - val_loss: 18.7274\n",
      "Epoch 61/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3662\n",
      "Epoch 61: loss improved from 0.37215 to 0.36621, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 197s 289ms/step - loss: 0.3662 - val_loss: 18.8684\n",
      "Epoch 62/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3633\n",
      "Epoch 62: loss improved from 0.36621 to 0.36332, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 203s 299ms/step - loss: 0.3633 - val_loss: 19.1970\n",
      "Epoch 63/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3683\n",
      "Epoch 63: loss did not improve from 0.36332\n",
      "681/681 [==============================] - 205s 300ms/step - loss: 0.3683 - val_loss: 19.3089\n",
      "Epoch 64/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3651\n",
      "Epoch 64: loss did not improve from 0.36332\n",
      "681/681 [==============================] - 200s 293ms/step - loss: 0.3651 - val_loss: 19.1402\n",
      "Epoch 65/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3635\n",
      "Epoch 65: loss did not improve from 0.36332\n",
      "681/681 [==============================] - 431s 634ms/step - loss: 0.3635 - val_loss: 19.2692\n",
      "Epoch 66/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3598\n",
      "Epoch 66: loss improved from 0.36332 to 0.35984, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 434s 638ms/step - loss: 0.3598 - val_loss: 19.1278\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681/681 [==============================] - ETA: 0s - loss: 0.3524\n",
      "Epoch 67: loss improved from 0.35984 to 0.35240, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 416s 611ms/step - loss: 0.3524 - val_loss: 19.4115\n",
      "Epoch 68/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3518\n",
      "Epoch 68: loss improved from 0.35240 to 0.35181, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 445s 653ms/step - loss: 0.3518 - val_loss: 19.2334\n",
      "Epoch 69/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3505\n",
      "Epoch 69: loss improved from 0.35181 to 0.35049, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 811s 1s/step - loss: 0.3505 - val_loss: 19.0654\n",
      "Epoch 70/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3506\n",
      "Epoch 70: loss did not improve from 0.35049\n",
      "681/681 [==============================] - 199s 293ms/step - loss: 0.3506 - val_loss: 19.6935\n",
      "Epoch 71/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3462\n",
      "Epoch 71: loss improved from 0.35049 to 0.34618, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 212s 311ms/step - loss: 0.3462 - val_loss: 19.7683\n",
      "Epoch 72/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3458\n",
      "Epoch 72: loss improved from 0.34618 to 0.34583, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 223s 328ms/step - loss: 0.3458 - val_loss: 19.6874\n",
      "Epoch 73/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3480\n",
      "Epoch 73: loss did not improve from 0.34583\n",
      "681/681 [==============================] - 221s 325ms/step - loss: 0.3480 - val_loss: 19.8746\n",
      "Epoch 74/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3414\n",
      "Epoch 74: loss improved from 0.34583 to 0.34137, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 219s 322ms/step - loss: 0.3414 - val_loss: 19.8540\n",
      "Epoch 75/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3360\n",
      "Epoch 75: loss improved from 0.34137 to 0.33602, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 209s 307ms/step - loss: 0.3360 - val_loss: 19.7581\n",
      "Epoch 76/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3361\n",
      "Epoch 76: loss did not improve from 0.33602\n",
      "681/681 [==============================] - 212s 312ms/step - loss: 0.3361 - val_loss: 19.8693\n",
      "Epoch 77/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3341\n",
      "Epoch 77: loss improved from 0.33602 to 0.33413, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 274s 403ms/step - loss: 0.3341 - val_loss: 19.9182\n",
      "Epoch 78/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3342\n",
      "Epoch 78: loss did not improve from 0.33413\n",
      "681/681 [==============================] - 439s 645ms/step - loss: 0.3342 - val_loss: 20.4313\n",
      "Epoch 79/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3340\n",
      "Epoch 79: loss improved from 0.33413 to 0.33395, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 445s 654ms/step - loss: 0.3340 - val_loss: 20.1235\n",
      "Epoch 80/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3333\n",
      "Epoch 80: loss improved from 0.33395 to 0.33327, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 439s 645ms/step - loss: 0.3333 - val_loss: 20.1807\n",
      "Epoch 81/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3340\n",
      "Epoch 81: loss did not improve from 0.33327\n",
      "681/681 [==============================] - 440s 646ms/step - loss: 0.3340 - val_loss: 20.8143\n",
      "Epoch 82/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3303\n",
      "Epoch 82: loss improved from 0.33327 to 0.33034, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 450s 661ms/step - loss: 0.3303 - val_loss: 19.9532\n",
      "Epoch 83/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3255\n",
      "Epoch 83: loss improved from 0.33034 to 0.32545, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 230s 338ms/step - loss: 0.3255 - val_loss: 19.9069\n",
      "Epoch 84/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3237\n",
      "Epoch 84: loss improved from 0.32545 to 0.32368, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 196s 288ms/step - loss: 0.3237 - val_loss: 20.7460\n",
      "Epoch 85/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3254\n",
      "Epoch 85: loss did not improve from 0.32368\n",
      "681/681 [==============================] - 208s 306ms/step - loss: 0.3254 - val_loss: 20.5879\n",
      "Epoch 86/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3266\n",
      "Epoch 86: loss did not improve from 0.32368\n",
      "681/681 [==============================] - 222s 326ms/step - loss: 0.3266 - val_loss: 20.2663\n",
      "Epoch 87/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3202\n",
      "Epoch 87: loss improved from 0.32368 to 0.32023, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 230s 338ms/step - loss: 0.3202 - val_loss: 20.4083\n",
      "Epoch 88/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3226\n",
      "Epoch 88: loss did not improve from 0.32023\n",
      "681/681 [==============================] - 216s 317ms/step - loss: 0.3226 - val_loss: 20.5518\n",
      "Epoch 89/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 89: loss did not improve from 0.32023\n",
      "681/681 [==============================] - 225s 331ms/step - loss: 0.3262 - val_loss: 21.0719\n",
      "Epoch 90/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3190\n",
      "Epoch 90: loss improved from 0.32023 to 0.31897, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 228s 335ms/step - loss: 0.3190 - val_loss: 20.6669\n",
      "Epoch 91/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3155\n",
      "Epoch 91: loss improved from 0.31897 to 0.31547, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 227s 334ms/step - loss: 0.3155 - val_loss: 21.1928\n",
      "Epoch 92/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3129\n",
      "Epoch 92: loss improved from 0.31547 to 0.31286, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 226s 332ms/step - loss: 0.3129 - val_loss: 21.3225\n",
      "Epoch 93/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3124\n",
      "Epoch 93: loss improved from 0.31286 to 0.31238, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 223s 328ms/step - loss: 0.3124 - val_loss: 21.6062\n",
      "Epoch 94/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3231\n",
      "Epoch 94: loss did not improve from 0.31238\n",
      "681/681 [==============================] - 226s 332ms/step - loss: 0.3231 - val_loss: 20.4285\n",
      "Epoch 95/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3279\n",
      "Epoch 95: loss did not improve from 0.31238\n",
      "681/681 [==============================] - 231s 340ms/step - loss: 0.3279 - val_loss: 21.5383\n",
      "Epoch 96/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3157\n",
      "Epoch 96: loss did not improve from 0.31238\n",
      "681/681 [==============================] - 231s 340ms/step - loss: 0.3157 - val_loss: 20.7691\n",
      "Epoch 97/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3079\n",
      "Epoch 97: loss improved from 0.31238 to 0.30789, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 219s 322ms/step - loss: 0.3079 - val_loss: 21.2358\n",
      "Epoch 98/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3046\n",
      "Epoch 98: loss improved from 0.30789 to 0.30457, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 225s 331ms/step - loss: 0.3046 - val_loss: 21.1573\n",
      "Epoch 99/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3018\n",
      "Epoch 99: loss improved from 0.30457 to 0.30175, saving model to nextword.hdf5\n",
      "681/681 [==============================] - 228s 335ms/step - loss: 0.3018 - val_loss: 20.7623\n",
      "Epoch 100/100\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.3036\n",
      "Epoch 100: loss did not improve from 0.30175\n",
      "681/681 [==============================] - 221s 324ms/step - loss: 0.3036 - val_loss: 21.3567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ecb0f43460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwJUlEQVR4nO3dd5xU1fnH8c8zs41dlrZ0kCIqAhaQoojErog9KDbUJEY0mmgSNWqi8Zfyi8b8YkmxoBI7VlCiqFhRVLo0QUSQsoDsUpa+dc7vj3MXFlhwF3b27s5836/Xvnbmzr1zn7sDzz3z3HPPMeccIiKSPCJhByAiIrVLiV9EJMko8YuIJBklfhGRJKPELyKSZJT4RUSSjBK/yF6Y2ZNm9ucqrrvEzE7Z3/cRiTclfhGRJKPELyKSZJT4pd4LSiy3mNlsM9tiZk+YWSsze8vMNpnZe2bWtML655jZl2ZWYGYfmVm3Cq/1MrMZwXYvAhm77OssM5sZbPuZmR2xjzFfbWbfmNk6MxtrZm2D5WZm95tZnpltNLM5ZnZY8NpgM5sXxLbCzG7epz+YJD0lfkkUQ4BTgUOAs4G3gN8CLfD/zm8AMLNDgFHAL4PXxgH/NbM0M0sDXgOeAZoBLwfvS7BtL2AkcA2QAzwKjDWz9OoEamYnAXcDQ4E2wFLgheDl04AfBMfROFhnbfDaE8A1zrls4DDgg+rsV6ScEr8kin8651Y751YAnwCTnXNfOOcKgTFAr2C9i4A3nXPvOudKgP8DGgDHAscAqcADzrkS59wrwNQK+xgOPOqcm+ycK3POPQUUBdtVx2XASOfcDOdcEXA70N/MOgElQDZwKGDOufnOuVXBdiVAdzNr5Jxb75ybUc39igBK/JI4Vld4vK2S5w2Dx23xLWwAnHMxYDnQLnhthdt55MKlFR53BG4KyjwFZlYAHBBsVx27xrAZ36pv55z7APgX8G8gz8xGmFmjYNUhwGBgqZlNMLP+1dyvCKDEL8lnJT6BA76mjk/eK4BVQLtgWbkOFR4vB/7XOdekwk+mc27UfsaQhS8drQBwzv3DOdcb6I4v+dwSLJ/qnDsXaIkvSb1Uzf2KAEr8knxeAs40s5PNLBW4CV+u+Qz4HCgFbjCzVDP7IdCvwraPAdea2dHBRdgsMzvTzLKrGcMo4Mdm1jO4PvAXfGlqiZn1Dd4/FdgCFAKx4BrEZWbWOChRbQRi+/F3kCSmxC9JxTm3ABgG/BNYg78QfLZzrtg5Vwz8EPgRsA5/PWB0hW2nAVfjSzHrgW+Cdasbw3vAncCr+G8ZXYCLg5cb4U8w6/HloLXA34LXLgeWmNlG4Fr8tQKRajNNxCIiklzU4hcRSTJK/CIiSUaJX0QkySjxi4gkmZSwA6iK5s2bu06dOoUdhohIvTJ9+vQ1zrkWuy6vF4m/U6dOTJs2LewwRETqFTNbWtlylXpERJKMEr+ISJJR4hcRSTL1osYvIlJdJSUl5ObmUlhYGHYocZeRkUH79u1JTU2t0vpK/CKSkHJzc8nOzqZTp07sPOBqYnHOsXbtWnJzc+ncuXOVtlGpR0QSUmFhITk5OQmd9AHMjJycnGp9s1HiF5GElehJv1x1j1OJX0RkfxVthuItYUdRZUr8IiL7yjnYvBrWLoT1S/zzCgpWLuahv/1pt+XfZ/DgwRQUFNRcnLtQ4hcR2RcuBhuWw8aVEE2DsmIoK9pplYKV3/LQE0/t9m2gtLR0r289btw4mjRuDEWbqn3SqAolfhGR6oiVwda1sGah/92wFeR08a8VbtqxXmkxt/3pXhYtzaVnn3707duXgQMHcs4559C9e3dwMc477zx69+5Njx49GDFixPZNO3XqxJoV37Jk5sd069aVq6++mh49enDaaaexbdu2/T4EdecUkYT3h/9+ybyVG/f/jcqKoawEcHRvmcFdZ/eAzGb+tWg6FG2EhsGYaEUbuOe3NzD362+Z+c5zfDQ/nzPPOZ+5c+fSuXkDWDWLkX+9jWat27EtlkrfgacwZMgQcnJy/PabVkEkjYXfLGbUCy/x2GOPMXToUF599VWGDRu2X4ehxC8isqtYCVgUrEJRJFbqE38k6ks7DZruSPoAGdmwdR3EYhCJQOFGiKRCNLipqnAD/fr1o3O7lrDma0jL4h9PjGTMm+PBOZav+I6FCxf6xO9ivmyU3YrOnTvTs2dPAHr37s2SJUv2+/CU+EUk4d11do+qr7x1LRQs88m9eVeIpvjyTv5X/kTQouvOJ4Ry6Y1gyxoo3gxpWb4+n54NGGQ0gcKNZGVm+ovA0TQ+mr2c9z77gs+nTCezKI8TzhpK4cY1/gQTK4PULEjLJj09ffsuotFojZR6VOMXkfqltNgn1XgoK4ENKyAlwz8u76mzaZVv7Tc+oPKkD5DWEDAfW9EmwJHdvA2bNm2CrBa+FV+82b9Pk45s2LSZpk2bkpnVkK++28akGXNg03dQsBxwkN0G4nQfglr8IlK/jL4avpsDP5/qyy6V2bLGJ9+ykh2llqrYuMIn6KadoGSrb/mv/xYKN0BmDqQ33PO2kahP/kUbwZWBRclp3YEBAwZwWO9jaJBqtMpp4i8Gpzdk0KBBPPLII3Tr1o2uXbtyzNHH+PJSYYE/uaRl+hNFHJiLQ1ehmtanTx+niVhEhNzp8PhJ/vGw0XDQyZWv9+bNzG92Kt06tvIJu2FLSEmvfN1yhRth3SJo2BoatfHLNuTClnyIpEDLbv733mxe7bt3WtSXeZpVGDunaDNsWw+N2+35W0PxVt/qb9weUtL2vq9dzJ8/n27duu20zMymO+f67LquSj0iUn988EefyBs0hS+erXyd4q0w+yVIaeAvvm5dC3nzYVtB5es751v0Bcv8ySG71Y7XGrXzJ42mnb4/6YOv84Nv8Wc02uW1htBkL6Ui8K38nAOrnfSrS6UeEak7Zo7y3SEPOmX31xZPgMUfwel/8Ul62kjfi6ZizxqAea9D0Qbfy6ZJB9+CX7/El2xcxx3rx0r99lvyg946qdCk486J2cwn/6pKyfDvEyvZcRKog9TiF5G6Yeln8Nq18NyFMP2pnV9zDj74k0/Cfa6CXsN8sp7z8u7vM+MpaNbFJ2HwreecLr7+XrDUl2M2LIfVX/qafiTVt+hbdfe9cfaHGWQ19yeX6lxbqGVxS/xmdoCZfWhm88zsSzO7MVjezMzeNbOFwe+m8YpBROqJkkIY+wvfQu9yEvz3Bpj4gH+teCvMfB5yp8Lxv4HUDGh9OLQ5EmY8s/OQBvkLYNnncNQVO79/JArNDoS0bF+D37LWd7Fsfgi0OMSXjvZWgqmO7Nb+m0MdFs9STylwk3NuhpllA9PN7F3gR8D7zrl7zOw24Dbg1jjGISJ13cf3wtpv4PLXoOMAGHMNvHcXTH3cX2DFQc7B0POyHdv0uhzG3QyrZkHbnn7ZjKd9Lb7npbB87c77KE/+hQX+wmsdbpHHW9xa/M65Vc65GcHjTcB8oB1wLlD+Pe4p4Lx4xSAi9cCq2b513/My6HKiL80MeRxOuB3a9oLjb4WhT8NV43dO1odf4IdJmDYSykqhtMh/M+g62F+QrUwkUufLMLWhVi7umlknoBcwGWjlnFsVvPQd0GoP2wwHhgN06NChFqIUkf1WuAG+HOO7M2Y09iWUtr127v9etAkWvgt583wrf9kk31PntD/vWCcShRNu2/u+GjSFHuf5mv7M5/xNUtvWQe8fxePI9klBQQHPP/881113XbW3feCBBxg+fDiZmZk1HlfcE7+ZNQReBX7pnNtYcaYY55wzs0pvJHDOjQBGgO/HH+84RaSKthXAimmwfqlP6q2P8MtnPAkf/sV3n6womgYdj4VOx8HKmT7plxX5mnqTjtDqMBj4691751TFWfdD5x/AusX+J6UBHHjifh5gzSkoKOChhx7a58Q/bNiw+pf4zSwVn/Sfc86NDhavNrM2zrlVZtYGyItnDCJSQ5ZPhf/e6FvqVGiLpWX71veGZdDxODj1D77UUrgBNq2Gbz/yyf6DP/uulX1+DN3Pg3a997+/elqW7+FTR912220sWrSInj17cuqpp9KyZUteeuklioqKOP/88/nDH/7Ali1bGDp0KLm5uZSVlXHnnXeyevVqVq5cyYknnkjz5s358MMPazSuuCV+8037J4D5zrn7Krw0FrgSuCf4/Xq8YhCRGhKLwRu/8qWUE38HB/T1rfUV02Hpp7DuWxh0Nxx65s7jy7Q+HA4+xZdxtqz1J4hICL3I37rND/NQk1ofDmfcs9dV7rnnHubOncvMmTMZP348r7zyClOmTME5xznnnMPHH39Mfn4+bdu25c033wRgw4YNNG7cmPvuu48PP/yQ5s2b12zcxLfFPwC4HJhjZjODZb/FJ/yXzOwqYCkwNI4xiEhVlBb7fvF7Govmy9Gweg788HE44sIdy5t19hdZqyIrZ//jrMfGjx/P+PHj6dWrFwCbN29m4cKFDBw4kJtuuolbb72Vs846i4EDB8Y9lrglfufcRGBPQ8vtYYANEakRm77zrevvG58GfK3++Yt8t8kBN0L/63a+kamsxJdpWh0Ghw2JX8zx9D0t89rgnOP222/nmmuu2e21GTNmMG7cOO644w5OPvlkfv/738c1Ft25K5IoCpbBxPvhkYHw967wj14w9Qnfmi8t9kMZjLoURg/39XrnIHcaPH4ybFoJHfvDh38OtnvcbwPwxTN+uIOT7gynTFOPZWdn+2GZgdNPP52RI0eyebMfcXPFihXk5eWxcuVKMjMzGTZsGLfccgszZszYbduaprF6RBLBgrfhpct9uaZdHzjpDn9B9c1fwyf3QWkhbF0D2W39UL+zX/R3vuYv8HeaXjrO38G6bLK/cerNm3zf+oG/hgn3wgFHwyGnh32U9U5OTo4flvmwwzjjjDO49NJL6d+/PwANGzbk2Wef5ZtvvuGWW24hEomQmprKww8/DMDw4cMZNGgQbdu2rfGLuxqWWaQqYmUw5lrfg+TA48OOZmcL3oIXL4dWPWDoU37cGfAt+kUfwGf/8HeqHnWlHw6hZCvMesF/G8jM8dtkVbiA6Bwset93zVwx3S/70TjoNKDWD21/VDZMcSKrzrDMavGLVMWiD2DOS761vLfEP/tlP5ZMt7P3fV+lRfDN+7B6Lhz7C0htsOd1v3oTXrrS9zC5fAw0aLLjNTM/Xv2uY9anZ0O/q/1PZcz86JhdToaF4/3YNvUs6cveKfGLVMWMYJSRRR9Cybbdk3Es5kskn/3DPz/mOjj1T36+1j0pK4Wv/gtLJvrxZaJpfojgr8b5YYXB35R03sM7d5F0zg9PPGWEb+23O8pPSlIx6dcEM5V3EpQSv8j32ZzvE2ybI/2AYIsnQNdBO14vLYLXroO5r0Dfn/okPukh32988P/54QdKi/wY7bGYn6QjdypMesTf9JTeCDBfn09Jh25nwWE/hOVTYMJf/Z2x/YM7PxdPgLd+4yf+zmwOA2/yPXF2nfRDAN+TxuI0b21dUt2SvRK/JJ7SIj/nauNqTKCxN7NG+Uk7zvkX/GcwLBi3I/E750stX78Fp/wPDPilbym36envcn3o6D2/b4dj/U1PXc+ofO7YA0/yY8aPvwOadvTln2lP+BEmz3sEepzvy0pSqYyMDNauXUtOTk5CJ3/nHGvXriUjo+r/FpT4JfG8eRN8+Rr8cva+jf9SkXO+O+MBR0ObI3y9/Ot3fMs9EoFvP96R9I/71Y7tel7ihyTInepb8dE0/00gEvUDmDVq4y/G7k0kAuc/Ao+fAi9cChgcc73vsZNW8+O3JJr27duTm5tLfn5+2KHEXUZGBu3bt6/y+kr8kljWfONHanQxPyfrgBt2vLZhha/V97zMt6CrYvlkWPO1b+2DH/J33muw6gtoe5QvxWS3gaN/tvu2LYJJPvZHejZc/LzvYdNvOHTYyzcI2UlqaiqdO3f+/hWTkBK/7Nln//Q17XP+tXNJYfEE3+o96XfVm6ouFoPN3/k7RNOyoGEraNCsZm8K+uhuP+Ve84NhymPQ//odZZRxt8CCN32/9r5XwcCb/fyu4Gd5Wva5v2i68gvfGj/4NN/fPa2hL6sAHHyqH1VywVtQvMWPU3PGvfEtueR0gQueiN/7S9JR4pfKrZoF797lL0SWbIMLn/I9VBZ96G/vLyuC5ZPg0pd27gNemTUL/d2iefP8jUQVRdP8RBsDb9rRc2XrOnjtZ3643f7XV/6ehRtg8ghfe//BzX5ijdVfwtxXfcmlbS9/Q9OCt/zF0iUTfdI/5noo3uRPClNG+LILzr8P+PlXW3WH6U/C5Ef8sqOu2DGGTWYz6NDf3zC19HM/2uRRV+7LX1gkNEr8srtYmb8wmZnjywsf/hneuBGOuBhGXQI5B8GxP/ejNT5xGlwyCjbnwZJPYNMq35IuL6Vszodnh/jWcd+f+kG9GneAki3+tW8n+Em0izfDyXf5MWaeOR/y5/tvFS27+ZuOyhVtgsmP+m8jhQV+2ZJP4MInfTkkPdv3fU9vBI0P8Mm762B453d+ou6T7/RdMY+9AWa/5BO+mT8Bte/jk3palv8GsOQT/y2gz092/vt0PcNfcAUY9FddYJV6R3fuir8JqLTQj5EeicKkh+Ht22DIE37kxQ/+18+JahE/OfWVb/gSybLJ8PzQHQnYIj6Bpjbw23boD0+dBavnwY/ehPa9d993LAbjbvLT5/Ua5i+Wbl0PF4yEd+/0vXOunegvhuZOg5d/BBuWwyFn+Bma8hf4k1R6NmzJgxN+CycEUzhPfMD3rR9wI3z6IJz/KBx58f7/vdZ8A//q7Vv7N87c+w1WIiHa0527SvzJ7otn4fWgnNL8EDj6Wnj399DhGLjsFd8ads4vW/opXDwKsivMlrlmoZ9qr/XhPtFvXQsvDoO8+b5OvvpLuPg5P077njjnW9Cf/8vX/Ie96m9KyvsKHjvRl20OPcvH0KiNHxq44kXOVbP9Pou3wA1f7OjTvnUd3NcdSrf5PvhXf1Rz1xPG/sJ/Eymv/YvUQUr8srvZL8Poq+HAE+Coy2HC33yJJaUBXD9px5gv1VW8xbfC57zsL3wevfswtLtxzp9A2hzpL2aWm/UCjAm273omnPdvP9xwZfss3rL7JNtjb/A9ea58AzrHf5xzkbpEiV92iMX8uDOvXedb6Ze97PuFx2Lw1Ru+xXzgCfu3D+d8vb9R2/2P99MHITXTXyOo7o04W9f5EtEhp+1/HCL1jBK/+CQ48zmY9h9Yt8jflDRs9J5nXRKRek2jcya70mJ49Af+wugBx/gulD3Oq9oMTSKSUJT4k8XCd3zSv+A/fgAwEUlamkctWcx83t8p2+2csCMRkZAp8SeDzfl+Qo0jLtr7+PAikhSU+JPBnJf9Hao9Lw07EhGpA5T4k8Gs5/348C2TZ/5REdkzJf5E990c/9PzsrAjEZE6Qok/0c0c5UecPPyCsCMRkTpCiT+RFSyH2S/4aQL3dyYqEUkYSvyJKm++HzK5rNQPkywiElDiT0TLJsPIQX76wR+Pg7Y9w45IROoQdepONGsXwTPn+XlgLx+97yNsikjCUuJPJM75eWUtClf+Fxq3CzsiEamDVOpJJPNeh0Xvw0l3KOmLyB4p8SeKok3w9u1+Jqy+Pw07GhGpw1TqSRQf3QObVsLQpzUej4jslVr8iSBvvp8g/agr4YC+YUcjInWcEn995xy8daufRevku8KORkTqASX++m7BOPh2Apz4O8jKCTsaEakHlPjrs9IieOe30OJQ6POTsKMRkXoibonfzEaaWZ6Zza2w7H/MbIWZzQx+Bsdr/0lh0kOwfgkMuhuiqWFHIyL1RDxb/E8CgypZfr9zrmfwMy6O+09sm1bDx/8HXc+ELieFHY2I1CNxS/zOuY+BdfF6/6T30d1QWgin/SnsSESkngmjxv9zM5sdlIKahrD/+i//a5jxNPS5CnK6hB2NiNQztZ34Hwa6AD2BVcDf97SimQ03s2lmNi0/P7+Wwqsn3v8DpGbC8b8JOxIRqYdqNfE751Y758qcczHgMaDfXtYd4Zzr45zr06JFi9oLsq5bNgm+egMG3AhZzcOORkTqoVpN/GbWpsLT84G5e1pXKuEcvPt7aNga+l8XdjQiUk/FbVAXMxsFnAA0N7Nc4C7gBDPrCThgCXBNvPafkBa9D8snw1kPQFpW2NGISD0Vt8TvnLukksVPxGt/SWHGM5CZAz0vCzsSEanHdOdufbFtvR+e4fALISUt7GhEpB5T4q8v5o6GsmI48uKwIxGRek6Jv76YNQpadIM2PcOORETqOSX++mDNQsidCj0vAbOwoxGRek6Jvz6Y9QJYBI64KOxIRCQBKPHXdbEYzH7RD8SW3TrsaEQkASjx13VLPoENy+HIynrHiohUnxJ/XTdrFKQ3gkPPDDsSEUkQSvx1WdFmmDcWepwHqQ3CjkZEEoQSf102fyyUbIEjLw07EhFJIEr8ddnM56FpZ+hwTNiRiEgCUeKvqwqW+Qu7R6rvvojULCX+umr2i/63hmgQkRqmxF8XOQczR0HH46Bpx7CjEZEEo8RfF+VOhXWL1NoXkbhQ4q+LPv8XpDWE7ueGHYmIJCAl/rpm1SyY9zr0vx4yGoUdjYgkICX+uuaDP0NGE5/4RUTiQIm/Llk2GRaOhwE3QkbjsKMRkQSlxF9XOAcf/AmyWsLRmoNeROJHib+u+HaCv2Fr4E2QlhV2NCKSwJT464oJf4PsttD7R2FHIiIJTom/Llg+BZZOhGN/AakZYUcjIglOib8umHg/NGgKR10RdiQikgSU+MOWNx8WjIN+10B6w7CjEZEkoMQftk8fhNRM6Dc87EhEJElUKfGb2Y1m1si8J8xshpmdFu/gEl7BMpjzMhx1JWTlhB2NiCSJqrb4f+Kc2wicBjQFLgfuiVtUyWLSI/73sT8PNw4RSSpVTfzlM4EMBp5xzn1ZYZnsi1iZb+0fMggatw87GhFJIlVN/NPNbDw+8b9jZtlALH5hJYElE2FLHhx+QdiRiEiSSanielcBPYHFzrmtZtYM+HHcokoGc1/xQy8ffHrYkYhIkqlqi78/sMA5V2Bmw4A7gA3xCyvBlRbDvLHQdTCkZYYdjYgkmaom/oeBrWZ2JHATsAh4Om5RJbpFH0Bhgco8IhKKqib+UuecA84F/uWc+zeQHb+wEtzcV/2Y+weeGHYkIpKEqlrj32Rmt+O7cQ40swiQGr+wEljxVn+n7mFDICUt7GhEJAlVtcV/EVCE78//HdAe+FvcokpkC9+B4s0+8YuIhKBKiT9I9s8Bjc3sLKDQOacaf3U5B9OfhIatoNNxYUcjIkmqqkM2DAWmABcCQ4HJZqYrk9W1YBws/shPrRiJhh2NiCSpqtb4fwf0dc7lAZhZC+A94JU9bWBmI4GzgDzn3GHBsmbAi0AnYAkw1Dm3fl+Dr1dKtsHbt0GLbhqQTURCVdUaf6Q86QfWVmHbJ4FBuyy7DXjfOXcw8H7wPDlMfMAPyjb4bxDVdXERCU9VW/xvm9k7wKjg+UXAuL1t4Jz72Mw67bL4XOCE4PFTwEfArVWMof5a962fbOWwIdB5YNjRiEiSq1Lid87dYmZDgAHBohHOuTH7sL9WzrlVwePvgFZ7WtHMhgPDATp06LAPu6pD3v09RFLgtD+HHYmISJVb/DjnXgVerakdO+ecmbm9vD4CGAHQp0+fPa5X561bDPP/CwN/DY3ahh2NiMjeE7+ZbQIqS7qGz92Nqrm/1WbWxjm3yszaAHnfu0V9N+Ux34On79VhRyIiAnzPBVrnXLZzrlElP9n7kPQBxgJXBo+vBF7fh/eoP4o2wRfPQvfzoFGbsKMREQHiOOeumY0CPge6mlmumV2Fn7XrVDNbCJxCos/iNXMUFG2EY34WdiQiIttVucZfXc65S/bw0snx2medEovB5EegXR9o3yfsaEREtotbiz/pLXof1i2Co68NOxIRkZ0o8cfLpIegYWvofm7YkYiI7ESJPx5WzfKTrfS7WkMvi0ido8QfDxMfgLRs6PvTsCMREdmNEn9NW7sI5r0Gfa+CBk3CjkZEZDdK/DXt0wchkgrHXBd2JCIilVLir0kbV8GsUdDrMsje4zBEIiKhUuKvSZP+DbFSOPaGsCMREdkjJf6asjkPpo70Qy836xx2NCIie6TEX1Mm3AulhXB88swtIyL1kxJ/TVi3GKb/B466ApofFHY0IiJ7pcRfEz74X9+T5/jEn0xMROo/Jf79tWoWzH3Fj8CpoZdFpB5Q4t8fzsG7d0FGExhwY9jRiIhUiRL//vjgT7D4Qzjxt7pLV0TqDSX+fTXlMfjk79D7R9BveNjRiIhUmRL/vpj3Ooy7BboOhsF/B7OwIxIRqTIl/urKmw+vXg3t+8KQJyAat0nMRETiQom/OkqLYfTVkJ4NFz8PaZlhRyQiUm1qrlbHhL/Cd3N80m/YIuxoRET2iVr8VbV8Cky8D3oOg0PPDDsaEZF9psRfFVvXwZhroFF7GHR32NGIiOwXlXq+z+Y8ePo82LACrngNMhqFHZGIyH5R4t+bDbnw9LmwcSVc+iJ0PDbsiERE9ltCl3piMcfydVv3beMV02HkGb7Ff/kY6HJizQYnIhKShE789z0/lsceupfCkrKqb1RaBO/9Dzx+ip9N68qx0OGYuMUoIlLbErrUc0XsdVqWvsqM5zZz1BX3QqSS81wsBqvn+huz8ubBgnGw5mvoNQxO/wtkNK79wEVE4iihE3/LSx9hwv1bOH7JY5S8lEfqkEchtYF/cctamPmcn0Bl3WK/LJIKLbvBZa/CwaeEF7iISBwldOInJY2cSx7lLw/fxu1fvQD/mAKpmX6KxM15ECuBDv1h4M3QrjfkdIFoathRi4jEVWInfuCw9k14pMdwfvZVBx5s/SXp6RmQkgFZzeGIi6BV97BDFBGpVQmf+AFuOq0rp8z9jruzz+V/zukRdjgiIqFK6F495To3z+KivgfwzKSlTFq8NuxwRERClRSJH+C2Mw6lY04m1z83gxUF28IOR0QkNEmT+BtlpPLYFX0oLo1xzTPTqte3X0QkgSRN4gfo0qIhD1zcky9XbuTWV2dTFnNhhyQiUuuSKvEDnNytFTef1pXXZ67kmmemsamwJOyQRERqVdIlfoDrTujCH8/twYcL8hny8GcsW7uP4/mIiNRDoSR+M1tiZnPMbKaZTQth/1zRvxNP/6QfqzcWcc6/J/L23FW1HYaISCjCbPGf6Jzr6ZzrE1YAAw5qzmvXD6B90wZc++wMfv3STDaq9CMiCS4pSz0VdW6exeifDeCGkw7i9ZkrGXT/x0z4Oj/ssERE4iasxO+A8WY23cyGV7aCmQ03s2lmNi0/P76JOC0lwq9P68or1/YnIy3KlSOncNNLsyjYWhzX/YqIhMGcq/0ujWbWzjm3wsxaAu8Cv3DOfbyn9fv06eOmTaudSwGFJWX864NveGTCIppkpvK/5x/O6T1a18q+RURqkplNr6ycHkqL3zm3IvidB4wB+oURR2UyUqPcfHpXxv78OFo1yuCaZ6bzm1dmqduniCSMWk/8ZpZlZtnlj4HTgLm1Hcf36d62EWOuG8D1J3bhlem5nPHgJ8xYtj7ssERE9lsYLf5WwEQzmwVMAd50zr0dQhzfKy0lwi2nH8rL1/bHDIY+8jlPTPyWMMpjIiI1JZQaf3XVZo1/TzZsLeHmV2bx7rzVnN6jFX+78EgaZWjSFhGpu+pUjb8+apyZyojLe3PHmd14f34eQx76jOXrdMeviNQ/SvzVYGb8dOCBPH1VP1ZvLOT8hz5V3V9E6h0l/n1wbJfmjL5uAJlpKVw8YhLj5mi4BxGpP5T499FBLRvy2vUDOLxdY65/fgbPTFoadkgiIlWixL8fmmWl8exVR3NS15bc+dpc7nv3a/X4EZE6T4l/PzVIi/Lo5b25sHd7/vH+Qn47Zg6lZbGwwxIR2aOUsANIBCnRCPdecAQtstN56KNF5G8q5p+X9KJBWjTs0EREdqMWfw0xM34z6FD+eG4P3v9qNZc9Pon1WzTIm4jUPUr8NeyK/p14+LKjmLtyI0Me/owla7aEHZKIyE6U+ONg0GFteP6nR7N+azHnP/QpU5esCzskEZHtlPjjpE+nZoy5bgBNM9O47LHJjPkiN+yQREQAJf646tQ8i9HXHctRHZvwqxdn8ec35qnHj4iETok/zppkpvHMVUdzZf+OPD7xW678zxTW6aKviIRIib8WpEYj/OHcw7j3giOY+u16zv7nRKYv1Rg/IhIOJf5aNLTPAbx8bX8iEbjo0c95dMIiYjHd6SsitUuJv5YdeUAT3vjFQE7t3oq73/qKnzw1lfxNRWGHJSJJRIk/BI0bpPLQZUfxx3N78NmitZzx4Md8+FVe2GGJSJJQ4g+JmXFF/0789+fH0bxhOj9+ciq/f30u24rLwg5NRBKcEn/IurbO5rXrB/CTAZ15+vOlnPnPT5i1vCDssEQkgSnx1wEZqVF+f3Z3nr3qaLYVl/HDhz/jgfe+pkR9/kUkDpT465DjDm7O27/8AWcf0YYH3lvI2f+cyOzcgrDDEpEEo8RfxzRukMoDF/dixOW9Wb+1mPP+/Sl/GTefwhLV/kWkZijx11Gn9WjN+F8dz0V9D2DEx4sZ9MDHTF68NuywRCQBKPHXYY0bpHL3D4/g+Z8eTZlzXDRiEne+NpeNhSVhhyYi9ZgSfz1w7EHNeeeXP+AnAzrz7OSlnPz3CYydtVLz+4rIPlHirycy01L4/dndef36AbRulMENo75g2BOT+Xr1prBDE5F6Rom/njmifRNeu34Afzq3B3NyNzDogY/53Zg5rNmsYR9EpGqU+OuhaMS4vH8nJtxyIlf078SLU5dzwt8+4sH3FrJJ9X8R+R5WH+rEffr0cdOmTQs7jDprUf5m7n37K975cjVNMlO55gddGHZMB7IzUsMOTURCZGbTnXN9dluuxJ84ZucWcN+7X/PRgnyy0qIM6d2eK/p35KCW2WGHJiIhUOJPIjOXF/D0Z0t4Y/Yqisti9OvUjAt6t2fwEW1omJ4SdngiUkuU+JPQ2s1FvDhtOa9My2Xxmi00SI1yQtcWHH9IC47v2oI2jRuEHaKIxJESfxJzzjFjWQGjZ+TywVd5rNpQCMCBzbPo26kZfTo1pecBTejUPIvUqK73iySKPSV+fe9PAmZG745N6d2xKc45FuZt5qMFeUxevI63v/yOF6ctByA1ahzYvCFdWmZxQNNM2jfLpH3TBrRr0oA2jTN0sVgkQajFn+RiMX8imLdqAwu+28zXqzexZM0Wctdvo3iXYaGz0qI0yUyjSWYqTTJTaZQR/DRIoWlWGs2C1xqmp5KVHqVhegoZqVHSUyNkpEbJSImSGjXMLKSjFUkuavFLpSIRo2vrbLq23rnnTyzmyNtURO76razcUMiqgm3kbSpi/dZiCraWULC1mLyNRWwsLGHDthIKS6o2d0DEoEFqlMz0FBqmp5CV7k8I6akR0lOipKcEJ4nUCGnRCCnRCKnRCKlRIxoxomakpkRoEKyTnhIlLcWvm5YSISVqpEYjpET8CSZiEDHbvm5qNIIZGP6bUFo0QmrKztuIJDolfqlUJGK0bpxB68YZVVp/W3EZ67cWs25LMVuLy9hSVMrmolIKS8ooLI1RVFLmH5fE2FpcxraSUjYX+fWKSssoKomxYVsJRSUxCkv9eiVlMUpKY5SUOUpjMWK18OU0JWKkRI2USISI+ZvlImZEgpNOStRIT4mQlhIlGgHDn1wwC04mEA3WT4n4k1VqNBL89u/rn0PMgXPgcH4fwUmq/IRVvm1ait+m/P2N8hiNlOiOOM2MWMxRFvyY+ZNbNPhd/jyyS4wp0QhpUdu+7/JTn1/f76+iaHBckeA9yznnryc52Ol4y/+e0Qg7nVjLT76RSvaxq/LNKhYoIhG2NwaikeAzi+z8nrEgnphzO21b8XMmeL0s5tcpCx6nBH//6n5LLS3b8W+2LOYwLGjYROpMw0KJX2pEg7QoDdIa0LZJ/HoKxWKO0pijpCy20wmluCxGcan/Kf8PV1rmcPj/yKUxR3FpjKJgnfLlzjlKyvz7FZfGKIm54D9tjLLYjmQQc27749IyR1HwXj6ZOJ/AYfugeTHn1ysujVEa2xFP6fb39+8VsZ0T4fbkE7xXaWzH++xadpPaVfHkGq1wkq54viqL+X8bZXtooZixvdyZlhLZ3riIBCdR//7+fV1wonLAPT88nKMPzKnR4wkl8ZvZIOBBIAo87py7J4w4pH6JRIy0iP9Pk5WE9yOUJ4OYc5QFJ5fSsh0nppjzCaq85QsQi/kWrNul5VtW4ZtBaSxGcanbqVW8/eQY7Lf8BOWCfZWW+Rh2BLejpQ3+/ctPpGXBCbs05nYbUTbmHLHvOaeVb1EehwXLYjG3vXW+PabYjhN++d+j4jcTC5Lq9pjKXJBsg29HkR1JPRY0DIpKY8SCfZSVfwaxXY6fHSXFjJRoUKL07xdzUFhS5r/1bm+g+J9Yhfcq/2xjFY4zYhaXThW1/r/HzKLAv4FTgVxgqpmNdc7Nq+1YROqT8nJNBCMFSMJzn9SQMDpt9wO+cc4tds4VAy8A54YQh4hIUgoj8bcDlld4nhss24mZDTezaWY2LT8/v9aCExFJdHX2Nk3n3AjnXB/nXJ8WLVqEHY6ISMIII/GvAA6o8Lx9sExERGpBGIl/KnCwmXU2szTgYmBsCHGIiCSlWu8X4JwrNbOfA+/gu3OOdM59WdtxiIgkq1A6hDnnxgHjwti3iEiyq7MXd0VEJD7qxeicZpYPLN3HzZsDa2ownPoiGY87GY8ZkvO4k/GYofrH3dE5t1u3yHqR+PeHmU2rbFjSRJeMx52MxwzJedzJeMxQc8etUo+ISJJR4hcRSTLJkPhHhB1ASJLxuJPxmCE5jzsZjxlq6LgTvsYvIiI7S4YWv4iIVKDELyKSZBI68ZvZIDNbYGbfmNltYccTD2Z2gJl9aGbzzOxLM7sxWN7MzN41s4XB76Zhx1rTzCxqZl+Y2RvB885mNjn4vF8MxoJKKGbWxMxeMbOvzGy+mfVP9M/azH4V/Nuea2ajzCwjET9rMxtpZnlmNrfCsko/W/P+ERz/bDM7qjr7StjEX2GmrzOA7sAlZtY93KjiohS4yTnXHTgGuD44ztuA951zBwPvB88TzY3A/ArP/wrc75w7CFgPXBVKVPH1IPC2c+5Q4Ej88SfsZ21m7YAbgD7OucPw43tdTGJ+1k8Cg3ZZtqfP9gzg4OBnOPBwdXaUsImfJJnpyzm3yjk3I3i8CZ8I2uGP9algtaeA80IJME7MrD1wJvB48NyAk4BXglUS8ZgbAz8AngBwzhU75wpI8M8aP6ZYAzNLATKBVSTgZ+2c+xhYt8viPX225wJPO28S0MTM2lR1X4mc+Ks001ciMbNOQC9gMtDKObcqeOk7oFVYccXJA8BvgPKpunOAAudcafA8ET/vzkA+8J+gxPW4mWWRwJ+1c24F8H/AMnzC3wBMJ/E/63J7+mz3K78lcuJPKmbWEHgV+KVzbmPF15zvs5sw/XbN7Cwgzzk3PexYalkKcBTwsHOuF7CFXco6CfhZN8W3bjsDbYEsdi+HJIWa/GwTOfEnzUxfZpaKT/rPOedGB4tXl3/1C37nhRVfHAwAzjGzJfgS3kn42neToBwAifl55wK5zrnJwfNX8CeCRP6sTwG+dc7lO+dKgNH4zz/RP+tye/ps9yu/JXLiT4qZvoLa9hPAfOfcfRVeGgtcGTy+Eni9tmOLF+fc7c659s65TvjP9QPn3GXAh8AFwWoJdcwAzrnvgOVm1jVYdDIwjwT+rPElnmPMLDP4t15+zAn9WVewp892LHBF0LvnGGBDhZLQ93POJewPMBj4GlgE/C7seOJ0jMfhv/7NBmYGP4PxNe/3gYXAe0CzsGON0/GfALwRPD4QmAJ8A7wMpIcdXxyOtycwLfi8XwOaJvpnDfwB+AqYCzwDpCfiZw2Mwl/HKMF/u7tqT58tYPhei4uAOfheT1Xel4ZsEBFJMolc6hERkUoo8YuIJBklfhGRJKPELyKSZJT4RUSSjBK/SJyZ2QnlI4iK1AVK/CIiSUaJXyRgZsPMbIqZzTSzR4Px/jeb2f3BePDvm1mLYN2eZjYpGAt9TIVx0g8ys/fMbJaZzTCzLsHbN6wwjv5zwV2oIqFQ4hcBzKwbcBEwwDnXEygDLsMPCjbNOdcDmADcFWzyNHCrc+4I/J2T5cufA/7tnDsSOBZ/Jyb4UVN/iZ8b4kD8eDMioUj5/lVEksLJQG9gatAYb4AfECsGvBis8ywwOhgXv4lzbkKw/CngZTPLBto558YAOOcKAYL3m+Kcyw2ezwQ6ARPjflQilVDiF/EMeMo5d/tOC83u3GW9fR3jpKjC4zL0f09CpFKPiPc+cIGZtYTtc512xP8fKR8F8lJgonNuA7DezAYGyy8HJjg/A1qumZ0XvEe6mWXW5kGIVIVaHSKAc26emd0BjDezCH6ExOvxk530C17Lw18HAD9E7iNBYl8M/DhYfjnwqJn9MXiPC2vxMESqRKNziuyFmW12zjUMOw6RmqRSj4hIklGLX0QkyajFLyKSZJT4RUSSjBK/iEiSUeIXEUkySvwiIknm/wG28E90/6ex7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"nextword.hdf5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "lstm = model.fit(X, y, validation_split=0.2, epochs=100, batch_size=128, shuffle=True, callbacks=[checkpoint]).history\n",
    "plt.plot(lstm['loss'])\n",
    "plt.plot(lstm['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22021f65",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0232169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : I am about\n",
      "Output :  I am about to\n",
      "\n",
      "Input : So I have\n",
      "Output :  So I have heard\n",
      "\n",
      "Input : To Clotilde Lothman\n",
      "Output :  To Clotilde Lothman von\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"nextword.hdf5\")\n",
    "tokenizer = pickle.load(open(\"tokenizer1.pkl\", 'rb'))\n",
    "\n",
    "def predict_next_words(model, tokenizer, text) :\n",
    "    \n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    preds = np.argmax(model.predict(sequence))\n",
    "    predicted_word = \"\"\n",
    "    \n",
    "    for key, value in tokenizer.word_index.items() :\n",
    "        if value == preds :\n",
    "            predicted_word = key\n",
    "            break\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "l = [\"I am about\", \"So I have\", \"To Clotilde Lothman\"]\n",
    "for i in l :\n",
    "    text = i\n",
    "    if text == \"0\" :\n",
    "        print()\n",
    "        print(\"Execution completed...\")\n",
    "        break\n",
    "    else :\n",
    "        try :\n",
    "            text = text.split(\" \")\n",
    "            text = text[-3:]\n",
    "            print(\"Input :\", i)\n",
    "            print(\"Output : \", i, predict_next_words(model, tokenizer, text))\n",
    "            print()\n",
    "            \n",
    "        except Exception as e :\n",
    "            print(\"Error occured : \", e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588f681",
   "metadata": {},
   "source": [
    "### Now using BI-LSTM which means not only forward pred, but backward too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5513cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3   4  https://towardsdatascience.com/databricks-how-...   \n",
       "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                  subtitle   image  claps responses  \\\n",
       "0                                      NaN   1.png    850         8   \n",
       "1                                      NaN   2.png   1100        11   \n",
       "2         A Grammar of Graphics for Python   3.png    767         1   \n",
       "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
       "4          One example of building neural…  5.jpeg    211         3   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             8  Towards Data Science  2019-05-30  \n",
       "1             9  Towards Data Science  2019-05-30  \n",
       "2             5  Towards Data Science  2019-05-30  \n",
       "3             4  Towards Data Science  2019-05-30  \n",
       "4             4  Towards Data Science  2019-05-30  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_data = pd.read_csv('medium_data.csv')\n",
    "medium_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9929dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  6508\n",
      "Number of fields:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", medium_data.shape[0])\n",
    "print(\"Number of fields: \", medium_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5d3ef92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove unwanted symbols\n",
    "medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "medium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))\n",
    "\n",
    "#Tokenize\n",
    "tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n",
    "tokenizer.fit_on_texts(medium_data['title'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d47ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<oov>:  1\n",
      "Strong:  4\n",
      "And:  8\n",
      "Consumption:  8237\n"
     ]
    }
   ],
   "source": [
    "print(\"<oov>: \", tokenizer.word_index['<oov>'])\n",
    "print(\"Strong: \", tokenizer.word_index['strong'])\n",
    "print(\"And: \", tokenizer.word_index['and'])\n",
    "print(\"Consumption: \", tokenizer.word_index['consumption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3731c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences:  48461\n"
     ]
    }
   ],
   "source": [
    "#Titles text into sequences and make n_gram model\n",
    "input_sequences = []\n",
    "for line in medium_data['title']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    #print(token_list)\n",
    "    \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# print(input_sequences)\n",
    "print(\"Total input sequences: \", len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e480cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 676,\n",
       "        68])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad sequences: making all same length \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b255c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f6aac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    5  676   68    2  452 1518]\n",
      "14\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(xs[5])\n",
    "print(labels[5])\n",
    "print(ys[5][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c8acd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramsu\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515/1515 [==============================] - 80s 50ms/step - loss: 6.7697 - accuracy: 0.1260\n",
      "Epoch 2/50\n",
      "1515/1515 [==============================] - 80s 53ms/step - loss: 5.7313 - accuracy: 0.1804\n",
      "Epoch 3/50\n",
      "1515/1515 [==============================] - 84s 56ms/step - loss: 4.8797 - accuracy: 0.2110\n",
      "Epoch 4/50\n",
      "1515/1515 [==============================] - 214s 141ms/step - loss: 4.1398 - accuracy: 0.2517\n",
      "Epoch 5/50\n",
      "1515/1515 [==============================] - 229s 151ms/step - loss: 3.5902 - accuracy: 0.3057\n",
      "Epoch 6/50\n",
      "1515/1515 [==============================] - 210s 139ms/step - loss: 3.2185 - accuracy: 0.3474\n",
      "Epoch 7/50\n",
      "1515/1515 [==============================] - 228s 150ms/step - loss: 2.9747 - accuracy: 0.3813\n",
      "Epoch 8/50\n",
      "1515/1515 [==============================] - 223s 147ms/step - loss: 2.8107 - accuracy: 0.4035\n",
      "Epoch 9/50\n",
      "1515/1515 [==============================] - 221s 146ms/step - loss: 2.6800 - accuracy: 0.4226\n",
      "Epoch 10/50\n",
      "1515/1515 [==============================] - 223s 147ms/step - loss: 2.6017 - accuracy: 0.4346\n",
      "Epoch 11/50\n",
      "1515/1515 [==============================] - 223s 147ms/step - loss: 2.5029 - accuracy: 0.4484\n",
      "Epoch 12/50\n",
      "1515/1515 [==============================] - 218s 144ms/step - loss: 2.4467 - accuracy: 0.4598\n",
      "Epoch 13/50\n",
      "1515/1515 [==============================] - 219s 145ms/step - loss: 2.4083 - accuracy: 0.4638\n",
      "Epoch 14/50\n",
      "1515/1515 [==============================] - 223s 147ms/step - loss: 2.3775 - accuracy: 0.4695\n",
      "Epoch 15/50\n",
      "1515/1515 [==============================] - 227s 150ms/step - loss: 2.3477 - accuracy: 0.4739\n",
      "Epoch 16/50\n",
      "1515/1515 [==============================] - 227s 150ms/step - loss: 2.2939 - accuracy: 0.4829\n",
      "Epoch 17/50\n",
      "1515/1515 [==============================] - 232s 153ms/step - loss: 2.2918 - accuracy: 0.4842\n",
      "Epoch 18/50\n",
      "1515/1515 [==============================] - 228s 151ms/step - loss: 2.2964 - accuracy: 0.4872\n",
      "Epoch 19/50\n",
      "1515/1515 [==============================] - 242s 159ms/step - loss: 2.2573 - accuracy: 0.4934\n",
      "Epoch 20/50\n",
      "1515/1515 [==============================] - 244s 161ms/step - loss: 2.2166 - accuracy: 0.5003\n",
      "Epoch 21/50\n",
      "1515/1515 [==============================] - 244s 161ms/step - loss: 2.1988 - accuracy: 0.5008\n",
      "Epoch 22/50\n",
      "1515/1515 [==============================] - 245s 162ms/step - loss: 2.2030 - accuracy: 0.4995\n",
      "Epoch 23/50\n",
      "1515/1515 [==============================] - 245s 162ms/step - loss: 2.1944 - accuracy: 0.5028\n",
      "Epoch 24/50\n",
      "1515/1515 [==============================] - 243s 160ms/step - loss: 2.1487 - accuracy: 0.5117\n",
      "Epoch 25/50\n",
      "1515/1515 [==============================] - 246s 162ms/step - loss: 2.1330 - accuracy: 0.5144\n",
      "Epoch 26/50\n",
      "1515/1515 [==============================] - 236s 156ms/step - loss: 2.1567 - accuracy: 0.5095\n",
      "Epoch 27/50\n",
      "1515/1515 [==============================] - 246s 162ms/step - loss: 2.1720 - accuracy: 0.5078\n",
      "Epoch 28/50\n",
      "1515/1515 [==============================] - 247s 163ms/step - loss: 2.1549 - accuracy: 0.5136\n",
      "Epoch 29/50\n",
      "1515/1515 [==============================] - 237s 156ms/step - loss: 2.1318 - accuracy: 0.5168\n",
      "Epoch 30/50\n",
      "1515/1515 [==============================] - 141s 93ms/step - loss: 2.1732 - accuracy: 0.5086\n",
      "Epoch 31/50\n",
      "1515/1515 [==============================] - 89s 58ms/step - loss: 2.0891 - accuracy: 0.5235\n",
      "Epoch 32/50\n",
      "1515/1515 [==============================] - 95s 63ms/step - loss: 2.1137 - accuracy: 0.5232\n",
      "Epoch 33/50\n",
      "1515/1515 [==============================] - 100s 66ms/step - loss: 2.1056 - accuracy: 0.5217\n",
      "Epoch 34/50\n",
      "1515/1515 [==============================] - 106s 70ms/step - loss: 2.1021 - accuracy: 0.5233\n",
      "Epoch 35/50\n",
      "1515/1515 [==============================] - 106s 70ms/step - loss: 2.0677 - accuracy: 0.5307\n",
      "Epoch 36/50\n",
      "1515/1515 [==============================] - 109s 72ms/step - loss: 2.0633 - accuracy: 0.5293\n",
      "Epoch 37/50\n",
      "1515/1515 [==============================] - 112s 74ms/step - loss: 2.0663 - accuracy: 0.5315\n",
      "Epoch 38/50\n",
      "1515/1515 [==============================] - 109s 72ms/step - loss: 2.0260 - accuracy: 0.5370\n",
      "Epoch 39/50\n",
      "1515/1515 [==============================] - 109s 72ms/step - loss: 2.0006 - accuracy: 0.5404\n",
      "Epoch 40/50\n",
      "1515/1515 [==============================] - 111s 73ms/step - loss: 2.0123 - accuracy: 0.5393\n",
      "Epoch 41/50\n",
      "1515/1515 [==============================] - 112s 74ms/step - loss: 2.0636 - accuracy: 0.5315\n",
      "Epoch 42/50\n",
      "1515/1515 [==============================] - 132s 87ms/step - loss: 2.0074 - accuracy: 0.5442\n",
      "Epoch 43/50\n",
      "1515/1515 [==============================] - 100s 66ms/step - loss: 2.0186 - accuracy: 0.5413\n",
      "Epoch 44/50\n",
      "1515/1515 [==============================] - 107s 70ms/step - loss: 1.9985 - accuracy: 0.5448\n",
      "Epoch 45/50\n",
      "1515/1515 [==============================] - 115s 76ms/step - loss: 2.0076 - accuracy: 0.5415\n",
      "Epoch 46/50\n",
      "1515/1515 [==============================] - 203s 134ms/step - loss: 1.9795 - accuracy: 0.5473\n",
      "Epoch 47/50\n",
      "1515/1515 [==============================] - 283s 187ms/step - loss: 1.9609 - accuracy: 0.5509\n",
      "Epoch 48/50\n",
      "1515/1515 [==============================] - 283s 187ms/step - loss: 1.9672 - accuracy: 0.5516\n",
      "Epoch 49/50\n",
      "1515/1515 [==============================] - 283s 187ms/step - loss: 2.0009 - accuracy: 0.5471\n",
      "Epoch 50/50\n",
      "1515/1515 [==============================] - 274s 181ms/step - loss: 1.9957 - accuracy: 0.5459\n",
      "<keras.engine.sequential.Sequential object at 0x000001ECB0EF4CA0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ecbe7d6460>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=50, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy vs loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# def plot_graphs(history, string):\n",
    "#     plt.plot(history.history[string])\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(string)\n",
    "#     plt.show()\n",
    "# plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26daab87",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39d99274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When i work in python make it is unhealthy\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"When i work in python\"\n",
    "next_words = 4\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted=np.argmax(predicted,axis=1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff021855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
